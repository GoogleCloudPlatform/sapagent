## HA Cluster Node is Fenced

Look for the keyword `Fence (reboot)` in the logparser output or cloud logging.

### Common Reasons for Fencing:

* **Fencing reason: Node left the cluster unexpectedly due to Corosync communication failure.**

  * Example: `2021-03-26 03:10:38 node1 pengine: notice: LogNodeActions:    * Fence (reboot) node2 'peer is no longer part of the cluster'`

* **Fencing reason: A cluster resource failed to stop**

  * Example: `2023-03-24 01:33:44 node1 pengine notice: LogNodeActions:  * Fence (reboot) node2 'rsc_SAPHanaTopology_SID_HDB00:1 failed there'`

* **Fencing reason: A cluster node is unclean**

  * Example: `2022-08-04T00:45:05.322022+00:00 node2 pengine[2872]:   notice:  * Fence (reboot) node1 'node is unclean'`

### Logs Indicating Fencing Action Results

* **Successful fencing:**

  * Example: `2021-03-26 03:10:57 node1 stonith-ng: notice: remote_op_done:    Operation 'reboot' targeting node1 on node2 for crmd.2569@node1.9114cbcc: OK` Â

* **Fencing timed out:**

  * Example: `2020-12-21 12:15:18 node2 stonith-ng:notice: remote_op_done:        Operation reboot of node1 by node2 for crmd.126900@node1.a3ccb74f: Timer expired` Â

### Diagnostic Steps

* **Fencing timeout:**
    * The fencing timeout is set to 300 seconds by default (configurable via `pcmk_reboot_timeout` or `stonith-timeout`). If the reset API takes longer than this, it times out. Pacemaker will retry the reset; check if subsequent resets are successful.
    * Sometimes the reset operation may encounter an error but still display the "Timer expired" error. Check the `stderr` output from `fence_gce` in the logs.
    * Continue to follow the decision tree to diagnose the root cause of the fencing action further.

## Corosync Communication Failure

Corosync is the communication layer for Pacemaker. It uses the totem protocol for "heartbeat"-like monitoring of the other node's health. A token is passed around to each node; the node performs some work and then passes the token to the next node. This process continues in a loop.

In GCP, the recommended Corosync `TOKEN` timeout is 20000 milliseconds (ms), and the `consensus` timeout defaults to 1.2 times the `TOKEN` value, which is 24000 ms. If the token is not received within 20 + 24 = 44 seconds, Corosync will declare the node as lost, and Pacemaker will initiate a fence action.

### Errors Indicating Corosync Communication Failure

* Example 1: `2022-07-09 05:26:44 node1 corosync [TOTEM ] A processor failed, forming new configuration: token timed out (20000ms), waiting 24000ms for consensus`
* Example 2: `2022-07-09 05:27:08 node1 corosync [TOTEM ] Failed to receive the leave message. failed: 1`

### Diagnostic Steps for Corosync Communication Failure

* Check the cloud logging of the failed VM for any `hostError`
* Go to the viceroy of the failed VM and check for any errors, live migration, packet drops, etc.
* Check if any relevant `omg` exists
* Check the Corosync configuration
* Check if the same errors are found for this known retransmit issue
* If both nodes tried to fence each other around the same time, check if they encountered the known "fence race" issue.

## Cluster Resource Failed to Stop

Pacemaker triggers a monitor operation for each cluster resource at a defined interval. If the monitor operation fails or times out, Pacemaker will attempt to recover the resource by first triggering a stop operation and then a start operation (except for HANA, where it will try to failover first).

When the cluster property `start-failure-is-fatal` is set to `true` (which is the default), and if a resource stop operation fails or times out and STONITH is enabled (recommended in GCP), the cluster will fence the node to start the resource elsewhere. If STONITH is not enabled, the cluster cannot continue and will not attempt to start the resource elsewhere but will retry stopping it after the failure timeout.

### Diagnostic Steps

* Check for any error messages in the system logs written by the resource agent around the failure time, explaining why it failed. For HANA, this might sometimes be due to timeouts calling HANA Python scripts.

  * Example:
    ```
    2023-01-06 20:45:14 node1 SAPHana(rsc_SAPHana_SID_HDB00)[159933]: WARNING: RA: HANA_CALL timed out after 60 seconds running command 'landscapeHostConfiguration.py'
    2023-01-06 20:46:35 node1 SAPHana(rsc_SAPHana_SID_HDB00)[159933]: WARNING: RA: HANA_CALL timed out after 60 seconds running command 'landscapeHostConfiguration.py'
    2023-01-06 20:46:35 node1 crmd info: process_lrm_event:    Result of monitor operation for rsc_SAPHana_SID_HDB00 on node2: 1 (unknown error)
    ```

* If this issue occurs with the SAPHanaToplogy resource agent in SUSE, check if the customer has encountered this known issue.

## HANA Failover

When the HANA monitor operation fails or times out, and if the HANA replication status is `SOK` (indicated by the `srHook` attribute `hana_[sid]_site_srHook_[sitename]` or `hana_[sid]_sync_state`) and `PREFER_SITE_TAKEOVER` is set to `TRUE` (recommended), Pacemaker will initiate a failover.

### Diagnostic Steps

* A possible cause is two timeouts calling the HANA Python script `landscapeHostConfiguration.py`.

* Starting from HANA 2.0 Rev 57, a runtimedump is automatically collected when one of the following scripts runs for a long time (default is longer than 30 seconds). Refer to SAP Note 2400007 - FAQ: SAP HANA Runtime Dumps.

  * `landscapeHostConfiguration.py`
  * `systemReplicationStatus.py`
  * `getTakeoverRecommendation.py`
  * `hdbnsutil -sr_state | -sr_stateConfiguration | -sr_stateHostMapping`

  * Ask the customer to check if a runtimedump named `[service]_[host].[port].rtedump.[timestamp].blocked_script.trc` is generated and reach out to SAP support.

* Another possible cause is the HANA Python script `landscapeHostConfiguration.py` returning 0 (Fatal) or 1 (Error). You might only see the monitor operation fail due to 1 (unknown error). The customer should reach out to SAP HANA support to investigate the HANA system error state.

  * Example: `2023-01-06 20:46:35 node1 crmd info: process_lrm_event:    Result of monitor operation for rsc_SAPHana_SID_HDB00 on node2: 1 (unknown error)`

## HANA Failed to Failover

The most common reason for HANA not failing over is that the replication sync state is `SFAIL`. In this case, Pacemaker will restart HANA locally. You should see logs similar to the following:

* Example: `2023-04-16T00:57:30.862817+05:30 node2 SAPHana(rsc_SAPHana_SID_HDB00)[26233]: ERROR: ACT: HANA SYNC STATUS IS NOT 'SOK' SO THIS HANA SITE COULD NOT BE PROMOTED`

### Verification and Diagnostic Steps

* Verify if the `srHook` attribute has a value. Check `/var/log/messages` to see if `SRHOOK1` has a value. In newer SLES versions, you should see logs indicating that the `srHook` attribute has the value `SOK`:

  * Example 1: 2023-03-23T16:00:01.942216-04:00 node2 SAPHana(rsc_SAPHana_BHP_HDB20)[42445]: INFO: DEC: hana_bhp_site_srHook_SiteB=SOK`
  * Example 2: `2023-03-23T16:00:01.968234-04:00 node2 SAPHana(rsc_SAPHana_BHP_HDB20)[42445]: INFO: DEC: Finally get_SRHOOK()=SOK`

* If the `srHook` attribute is empty, it falls back to the `hana_[sid]_sync_state` attribute. This attribute is updated during the SAPHana resource agent monitor operation by calling `systemReplicationStatus.py`, which has a hardcoded 5-second timeout. When it times out, `SFAIL` is set.

  * Example:
    ```
    Mar 28 02:03:26 node1 SAPHana(SAPHana_SID_00)[613948]: WARNING: HANA_CALL timed out after 5 seconds running command 'systemReplicationStatus.py --site=DC2'
    Mar 28 02:03:26 node1 SAPHana(SAPHana_SID_00)[613948]: INFO: DEC analyze_hana_sync_statusSRS systemReplicationStatus.py (to site 'DC2')-> 124
    Mar 28 02:03:26 node1 pacemaker-attrd[7042]: notice: Setting hana_sid_sync_state[node2]: SOK -> SFAIL
    ```

* **Recommendation:** Customers should install `srHook` to get accurate HANA replication status. Follow the solution section in the relevant issue.

* Check for any `cli-*` location constraint in the cluster configuration (refer to the associated issue).

* Check if customers encountered this issue if neither HANA node could be promoted.

## ASCS/ERS Restart or Failover

In an ASCS/ERS HA cluster, the cluster resource default parameter `migration-threshold` is set to 3. This means that if any cluster resource, such as ASCS or ERS, fails 3 times, the resource will be started on another cluster node.

The resource for ASCS has a metadata attribute `failure-timeout` set to 60, which means that the failcounts will be reset within 60 seconds.

Therefore, customers may observe either an ASCS/ERS restart or an ASCS/ERS failover, depending on the number of failures.

### Two Common Causes Triggering ASCS/ERS Restart/Failover

* **ASCS/ERS monitor operation timeout:** Follow the provided link to continue the root cause analysis (RCA).

  * Example: `2023-04-02T01:03:32.000176+00:00 node1 lrmd[1899]:  warning: rsc_sap_SID_ASCS10_monitor_11000:29869 - timed out after 60000ms`

* **The resources Filesystem/IPaddr2/ILB Health check are in the same resource group as ASCS/ERS and fail:** When Pacemaker recovers the failed resource, the resources in the same group will be stopped and then started in order. The following example demonstrates a scenario where the VIP resource timed out, leading to the restart of the ERS and Filesystem resources.

  * Example:
    ```
    2021-09-13 10:19:06 node2 crmd error: process_lrm_event:  Result of monitor operation for rsc_vip_gcp_SID_ERS60 on node2: Timed Out | call=716 key=rsc_vip_gcp_SID_ERS60_monitor_60000 timeout=60000ms
    2021-09-13 10:19:06 node2 pengine notice: LogActions: Recover rsc_vip_gcp_SID_ERS60   (Started node2)
    2021-09-13 10:19:06 node2 pengine notice: LogActions: Restart rsc_fs_SID_ERS60    (Started node2)
    2021-09-13 10:19:06 node2 pengine notice: LogActions: Restart rsc_sap_SID_ERS60   (Started node2)
    ```

## STONITH Timeout

### Legacy Fence Device (gcpstonith) Monitor Operation Timeout

A customer observed the following failed resource actions for a fence device. The monitor operation failed due to a timeout. This occurred with a fence device managed by the fence agent `gcpstonith`.

* **Failed Resource Actions:**

  * Example: `* STONITH-usgcpcardbc1_monitor_300000 on usgcpcardbc2 'unknown error' (1): call=153, status=Timed Out, exitreason='', last-rc-change='Mon Dec 21 23:40:47 2020', queued=0ms, exec=60003ms`

### Root Cause

The `gcpstonith` fence agent's monitor operation calls `gcloud compute instances list` and filters for the VM it's running on. It first calls `compute.zone.list` to list all zones and then calls `compute.instances.list` for each zone. If one of these API calls times out or doesn't return within the defined timeout period, the monitor operation fails.

Pacemaker doesn't require the STONITH resource to be in the "started" state to be used. (See reference.) The monitor operation of the fence device is primarily used to verify that it can make API/gcloud calls. Pacemaker can usually recover from "failed" monitor operations by stopping and starting the fence device.

**NOTE:** The root cause explained here doesn't apply to the `fence_gce` fence device, as the zone is provided in the STONITH configuration.

If the customer is experiencing the same timeout error with `fence_gce`, check the following:

* Is PGA set up correctly?
Customers can apply the same workaround to reset the failcount automatically.

### Solution/workaround

**NOTE:** Share the public troubleshooting guide link to customers for this issue.

* For a long-term solution, the customer should switch the fence agent from `gcpstonith` to `fence_gce`, where the API call is sent only to the zone where the VM is located. Share this public feature request with customers to track updates.
* If frequent timeouts or errors occur, follow the associated issue to check the PGA configuration.

## Cluster Resource in Failed/Stopped State

If a resource has failed, a failure message appears when displaying the cluster status. Pacemaker will try to recover the resource by triggering stop and then start operations. Each operation failure increases the resource's failcount. If the failcounts reach the resource's `migration-threshold`, the node will not be allowed to run the resource, and the resource will migrate to another node.

For a fence device, due to location constraints, if it reaches the threshold or the start operation also fails after the monitor operation fails, the device will be in a stopped state as the cluster has nowhere to run it.

* Example: `2021-09-13 10:19:06 node2 check_migration_threshold: Forcing secondary-stonith away from node1 after 1000000 failures (max=5000)`

**NOTE:** If a resource fails to start, the failcount will be set to `INFINITY`, and Pacemaker will try to start the resource on another node.

* For HANA HA clusters, the `migration-threshold` is set to 5000.
* For ASCS/ERS HA clusters, the `migration-threshold` is set to 3.

Review the logs to understand the cause of the failed resource. Cleaning up failed resources will reset the failcounts and start the resource.

### Show Failcounts

* On all resources: `crm_mon --failcounts`
* For a specific resource on a node: `crm_failcount --query -r [resource] -N [node]`

### Cleanup Resources

* **SLES:** `crm resource cleanup [resource]`
* **RHEL:** `pcs resource cleanup [resource]`

### Cleanup Failed Fenced Action

* `stonith_admin --cleanup --history "*"`
